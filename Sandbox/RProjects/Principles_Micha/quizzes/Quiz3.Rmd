---
title: "Quiz3"
output: html_document
date: "2022-11-21"
---
Run to generate document including embedded R code and its outputs:

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prediction Using a Regression Line

We are given two data sources describing the affect that using cigarettes has on one's Blood.Pressure. Run to view the data:

```{r}

Blood.Pressure=c(86,145,198,129,168,90,122,60,142,110,153,110,116,168,181,210,130,70,188,196)

cigarettes=c(4,13,10,10,15,4,9,1,11,6,8,3,8,18,11,7,4,2,19,15)
```

In order to predict a person's Blood Pressure based on one's daily intake of cigarettes, we'll use a Linear Regression analysis.

Run to create the linear model and retrieve its coefficients:

```{r}
fit.Blood.Cigar <- lm(formula = Blood.Pressure ~ cigarettes)
fit.Blood.Cigar
```

The left-hand value is the intercept coefficient ('b hat'), whereas the right-hand value is the growth coefficient ('a hat').

Each of the coefficients can be retrieved separately as well:
```{r}
slope.Blood.Cigar <- fit.Blood.Cigar$coefficients[2]
slope.Blood.Cigar

intercept.Blood.Cigar <- fit.Blood.Cigar$coefficients[1]
intercept.Blood.Cigar

```

Now that we have the linear model, we'll predict the Blood Pressure of a person with a daily intake of 11 cigarettes:

```{r prediction}
11*slope.Blood.Cigar + intercept.Blood.Cigar

```

Since we are interested in summarizing the trend between two quantitative variables, we ask: what is the best fitting line? 

A line that fits the data "best" will be one for which the n prediction errors (or residual errors) — one for each observed data point — are as small as possible in some overall sense. 

One way to achieve this goal is to invoke the "least squares criterion," which says to "minimize the sum of the squared prediction errors." That is, to calculate the exact sum of the squared prediction errors for each possible line, and select the line that provides the minimal error value. We'd have to implement the above procedure for an infinite number of possible lines — clearly, an impossible task!

Fortunately, we have formulas for calculating the intercept and the growth slope for the so-called "best" line, often referred to as the "least squares regression line," or simply the "least squares line."  It is also sometimes called the "estimated regression equation." Incidentally, note that in deriving the above formulas, we made no assumptions about the data other than that they follow some sort of linear trend.

best.intercept = mean(y) − best.slope * mean(x)

We can see from these formulas that the least squares line passes through the point (mean(x), mean(y)).

### The Limits of Extrapolating Outside the Model's "Scope"
What is the meaning of the growth coefficient ( $\hat{a}$ )?
In general, we can expect the response ($\overline{y}$) to increase or decrease by $\hat{a}$ units for every one unit increase in ${x}$.

What is the meaning of the intercept coefficient ( $\hat{b}$ )?
In general, if the "scope of the model" includes ${x=0}$, then the intercept is the predicted mean response when ${x=0}$. Yet often, an intercept of zero does not hold any meaning. If for example, the ${x}$ denotes height and is used to predict weight, for ${x=0}$ we'd get a height of zero, which is meaningless.


# Goodness of Prediction

To get an idea of how precise future predictions would be, we need to know how much the observed responses (y) vary around the (unknown) population mean, calculated by inserting the observed x into the equation of our regression line.

In the following section we will introduce a few metrics used in order to evaluate to what extent the prediction is precise, or 'good'.

The first metric that comes to mind is the sample variance in the responses (denoted σ squared) which calculates how much our observations vary from the mean (denoted μ). The higher the variance, the lower the precision. 

The problem is that variance is a population parameter, and we will rarely know its true value. The best we can do is estimate it!

### Estimating Variance Using Mean Square Error (MSE)

This metric *estimates* the common variance of the many subpopulations.

To calculate, we start by estimating the unknown population mean (μ) using the regression line equation. Unfortunately, we don't know the actual value of the population intercept (β) and the population slope (α), and we'll need to estimate them as well. Doing so "costs us" two degrees of freedom. What does that mean? In each prediction, we'll effectively be using the same constant intercept and constant slope, disregarding the precise intercept and slope for a given observation. For this reason, in the MSE equation, we have to divide by n-2, and not n, because we estimated two parameters used to calculate the result.

Run to calculate the square-Root of MSE (aka RMSE) using R code:

```{r RMSE}
summary(fit.Blood.Cigar)$sigma
```


### Estimating Coefficient of Determination (R-Squared)
This metric determine the "strength" of the correlation between our regression variables. It quantifies how much variations in our chosen predictor variable (x) affect the estimated values reached by using the regression line (y).

As we will explain below, this metric encompasses three sub-metrics:

SSR, the "regression sum of squares" which quantifies how far the estimated sloped regression line is from the horizontal "no relationship line," the sample mean or the predicted mean.

SSE is the "error sum of squares" and quantifies how much all data points (the y values of all observations) vary around the predicted values (y hat value for each observation), which make up our estimated regression line.

SSTO is the "total sum of squares" and quantifies how much the data points (the y values of all observations) vary around their mean.

In short:
SSTO = SSE + SSR
SSTO is the variation in the response y
SSE are the contributions to SSTO due to random error
SSR are the contributions to SSTO due to the regression of y on x

Note that while random error may be considered an acceptable contributor to the variation of y, we are interested mainly in deviations derived from changes made in the x, as this indicates if x can predict changes in y or not.

For this purpose, the Coefficient of Determination metric estimates the portion of the SSR out of the whole SSTO.

In practice, we have two equations for calculating the Coefficient of Determination (R-Squared):

a. R.Squared = SSR/SSTO
b. R.Squared = 1 - (SSE/SSTO)

And again, we can use R code to calculate this metric:

```{r R-Squared}
summary(fit.Blood.Cigar)$r.squared
```

### (Pearson) Correlation Coefficient r

A **correlation analysis** provides information on the *strength* and *direction* of the linear relationship between two variables.
Calculation: ∑(deviations of Xs from mean.X)*∑(deviations of Ys from mean.Y).

#### How does Correlation differ from slope of Linear Regression?

A **simple linear regression** analysis estimates parameters in a linear equation that can be used *to predict* values of one variable based on the other. Its slope is calculated by:
a. taking the standardized squared variation of the explainable variable SD(y)
b. multiplying it by the correlation coefficient r(x,y) which indicates the strength and direction of the variation
c. 'neutralizing' the standardized contribution of the explanatory variable SD(x).

In some sense, the two give you the same information - they each tell you the strength of the linear relationship between an observed predictor (x) and its observed result (y). Nevertheless, each provides distinct information:

The correlation gives you a bounded measurement (-1≤r≤1) that can be interpreted *independently of the scale* of the two variables. The closer the estimated correlation is to ±1, the closer the two are to a perfect linear relationship. The regression slope, when isolated from the regression equation, does not tell you that piece of information.

The regression slope (α) gives a useful quantity *interpreted as the estimated change* in the expected value of an observed result (y) for a given observed value of the predictor (x). Specifically, α tells you the change in the expected value of y corresponding to a 1-unit increase in x. This information cannot be deduced from the correlation coefficient alone.

Therefore, the correlation coefficient and the regression slope only coincide when SD(y)=SD(x). That is, they only coincide when the two variables are on the same scale, in some sense. The most common way of achieving this is through standardization.

#### Correlation Coefficient and Goodness of Prediction

In terms of traditional statistics, the R-Squared metric is the squared value of the correlation between our given variables, hence its name, as correlation is often denoted by 'r(x,y)'.

We can demonstrate this equality using R code:
```{r Covariance-Squared}
round(summary(fit.Blood.Cigar)$r.squared) == round(cor(cigarettes, Blood.Pressure)**2)
```

Let's take this opportunity to review the basic characteristics of the R-Squared and the Correlation Coefficient measures:

* Since R-Squared is a proportion, it is always a fraction between 0 and 1. Therefore the Correlation Coefficient which equals √(R-Squared), is always a fraction between -1 and 1. 

* If R-Squared = 1, it means that all of the data points (observed y values) fall perfectly on the regression line (estimated y values). We call this a 'perfect linear relationship' between x and y. This means that we have perfect correlation between the predictor (x) and the predicted variable (y). In other words, changes in x account for all of the variation in y! Note that this situation occurs both whether our Correlation Coefficient equals 1 or (-1). You may hear some refer to this difference by specifically noting that the perfect linear relationship is positive or negative, respectively.

* If R-Squared = 0, it means that none of the observed data points fall perfectly on the estimated regression line and we can say there is no linear relationship between x and y (zero correlation). In this case, our estimated regression line will be perfectly horizontal, meaning our predicted value is a constant number that isn't affected by the predictor (x). In other words, changes in the x account for *none* of the variation in y, and therefore x isn't a good predictor for y.

* The estimated slope and the Correlation Coefficient r always share the same sign. If α is positive, then r takes a positive sign.

* If the estimated slope of the regression line (α) is 0, then the correlation coefficient r must also be 0, because changes in the predictor account for none of the variation in the estimated values around the average value.

* One advantage of the Correlation Coefficient r is that it is unit-less, allowing researchers to make sense of correlation coefficients calculated on different data sets with different units. 

Students often ask: "what's considered a large r-squared value?" It depends on the research area. Social scientists who are often trying to learn something about the huge variation in human behavior will tend to find it very hard to get r-squared values much above, say 25% or 30%. Engineers, on the other hand, who tend to study more exact systems would likely find an r-squared value of just 30% unacceptable. The moral of the story is to read the literature to learn what typical r-squared values are for your research area!

See [here](https://online.stat.psu.edu/stat462/node/95/) for more examples.

---
For more details on using R Markdown see [here](https://rmarkdown.rstudio.com/lesson-1.html).
