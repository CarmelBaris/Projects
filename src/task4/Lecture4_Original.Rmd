---
title: "Lecture4"
author: "Yuval Benjamini"
date: "4 June 2024"
output: html_document
---

# Preparing files 


# Discussion of saving and retrieving files 

Let's prepare the GC file: 

```{r}
load("/Users/yuvalbenjamini/Library/CloudStorage/Dropbox/Courses/Lab/Lab_52568_16/DNAseq/chr1_line.rda")
```

Better to do this with a function as well
```{r, eval = FALSE}
N= last_read
bin_size = 5000
n_bins = ceiling(N/bin_size)
let_count = matrix(nr = n_bins,nc = 4)
letters = c("A","T","C","G")
for (i in 1:n_bins){
  start_bin = 1+ (i-1)*bin_size
  end_bin = i*bin_size
  for (let in 1:4) {
    let_count[i,let] = sum(chr1_line[start_bin:end_bin] ==letters[let],na.rm=TRUE)
  }
}

GC_5K = let_count[,3]+let_count[,4]
```

Let's read the coverage (reads_5K) file

```{r, eval = FALSE}
fname= "/Users/yuvalbenjamini/Library/CloudStorage/Dropbox/Courses/Lab/Lab_52568_17/Lectures/reads_5K.rda"
load(fname)
```

From now on, we usually work with two vectors: 

- The GC_5K vector
- The reads_5K vector

Stored as numerics, they are both smaller than 1 mbyte.

So you can easily share them across computers.

```{r, eval = FALSE}
data_fname_5K= "/Users/yuvalbenjamini/Library/CloudStorage/Dropbox/Courses/Lab/Lab_52568_24/Lectures//reads_gc_5K.rda"
save(file = data_fname_5K, reads_5K, GC_5K)
```

# Discussion of saving and retrieving files 

From now on, we usually work with two vectors: 

- The GC_xK vector
- The reads_xK vector

Stored as numerics, they are both smaller than 1 mbyte.
```{r, eval = FALSE}
format(object.size(reads_5K),units = "MB")
```

So you can easily share them across computers.

```{r, eval = FALSE}
save(file = filename_5K, reads_5K, GC_5K)
```

We can now clean our memory. 
```{r, eval = FALSE}
getwd()
## find the 10 largest objects in the base package
z <- sapply(ls(""), function(x)
            object.size(get(x, envir = baseenv())))
as.matrix(rev(sort(z))[1:10])

# Remove from memory
rm("reads_5K")
gc()

ls()  # list objects in the environment
rm(list = ls())  
gc()
```

## Read data

```{r}
data_fname_5K = "/Users/yuvalbenjamini/Library/CloudStorage/Dropbox/Courses/Lab/Lab_52568_24/Lectures//reads_gc_5K.rda"
load(data_fname_5K)
```

Make GC_5K into percentages
```{r}

if(any(GC_5K>1)){ # This makes sure I don't repeat the command
  GC_5K = GC_5K / 5000
}
```



## How to show scatter plots with multiple points?

First attempt

```{r}
plot(GC_5K,reads_5K)
```

Obviously problematic. Let's first fix the y axis. 
```{r}
plot(GC_5K,reads_5K,ylim = c(0,500))
```

### Adding transparent points

- The rgb command returns a color with 4 parameters:
      (red, green, blue, full)
- `full` default is 1
- The default color black has `(0,0,0,1)`
- Changing to `(0,0,0,0.5)` will make it half transparent
- `cex` changes size of point

```{r}
plot(GC_5K,reads_5K,col=rgb(0,0,0,0.2), ylim = c(0,500),cex = 0.5)
```


### Sampling points so it would be easier to see
```{r}
# impotant to set seed ! 
set.seed(100) 
samp = sample(length(GC_5K),1000)
samp[1:10]
plot(GC_5K[samp],reads_5K[samp], ylim = c(0,500),cex = 0.5)
```

Discussion:

- Why random sampling 
- Why set.seed?

### Density estimator (`smoothScatter`)

```{r}
smoothScatter(GC_5K,reads_5K, ylim = c(0,500))
```


## Linear regression

Assumed model: 
\[ Y_k = \beta_1 \cdot GC_k + \beta_0 + \epsilon_k,\qquad E[\epsilon_k] = 0  \]

Use the `lm()` command.

```{r}
reg_1 = lm(reads_5K~GC_5K)
# Bias term added by default

print(reg_1)
```

Discussion: 
- A '10%' increase in GC estimated to add ~45 reads. 
- What do we expect at 0? 

Additional information from the $lm()$ output 
accessed by `summary`:

```{r}
sum_reg_1 = summary(reg_1)
print(sum_reg_1)
```

We can see here: 

- The function call
- The distribution of the residuals
- Inference for model coefficients
- Anova statistics

Access to the coefficient inference:

```{r}
print(sum_reg_1$coefficients)

# Parameter estimates 
print(sum_reg_1$coefficients[,1])

# Std. Error
print(sum_reg_1$coefficients[,2])

# t statistics
print(sum_reg_1$coefficients[,3])

```

The lm has many built in functionalities.
Here are the saved fields
```{r}
names(reg_1)
```

The standard plots...

See http://data.library.virginia.edu/diagnostic-plots/
```{r}
plot(reg_1) 
```

How can we check the regression line?

1. Plotting residuals against the explanatory variable
```{r}
plot(reg_1$model$GC_5K,reg_1$residuals)
```

Again, with scaling so we can see something
```{r}
plot(reg_1$model$GC_5K,reg_1$residuals,ylim = c(-500,500),cex = 0.5, col = rgb(0,0,0,0.5))
```

Discussion: 
- What is the diagonal line? 
- Is this a good fit?

If there are many explanatory variables, we can plot against fitted values
```{r}
plot(reg_1$fitted.values,reg_1$residuals,ylim = c(-500,500),cex = 0.5, col = rgb(0,0,0,0.5))

```

2. Add a regression line to the original plot.

If the line is straight, can simply use the `abline` command
```{r}
plot(GC_5K,reads_5K,ylim = c(0,500), cex = 0.3 )
# In a-b line, intercept is a and slope is b
abline(a=reg_1$coefficients[1],b=reg_1$coefficients[2],col = 4, lw=3)
```

In cases where the line is not straight, we need to use the predict command:

1. Prepare sorted points for the "x" variable. The
data structure needs to be a list / data-frame with the correct variable name. 

```{r}
#We'll use regular intervals so the line would come out nicely
predict_data = data.frame(GC_5K = seq(0, 0.7, 0.01))
```

2. Use predict(model, predict_data) to get predictions. 

```{r}
predict_data$yhat = predict(reg_1, predict_data)
```

```{r}
plot(GC_5K,reads_5K,ylim = c(0,500), cex = 0.3 )

# lines only works well if x variable is sorted. 
lines(predict_data$GC_5K, predict_data$yhat, col=4, lw = 3)
```



3. Measure quantitatively using the residuals

Root mean squared error: 
\[rmse(\hat{Y}, Y) = \sqrt{\frac{1}{K}\sum_k (Y_k-\hat{Y}_k)^2} = \sqrt{\frac{1}{K}\sum_k r_k^2}\]

```{r}
reg_1_res = reg_1$residuals  # Can use this to estimate SD or MSE
rmse = sqrt(mean(reg_1_res^2) )
print(rmse)
```

But this may be strongly affected by outliers. What are more robust measures? 


## Outlier removal

### Removing first batch of outliers
```{r}
plot(GC_5K,reads_5K,ylim = c(0,500),cex = 0.5)

small_GCs = GC_5K < 0.1
abline(v = 0.1, col = 2 , lw= 4)
# How many are there? 
sum(small_GCs)
mean(small_GCs)
```

Where are they located along the chromosome?

We'll make a line of colors: 1 (black) for regular points, 2(red) for small_GCs

```{r}
col_line = rep(1, length(GC_5K))
col_line[small_GCs]=2
plot(GC_5K, col = col_line)
# We may consider removing only the ones near the center of the chromomsome
```

### Reestimate without small_GC outliers


```{r}
reg_2 = lm(reads_5K~GC_5K,subset = !small_GCs)

plot(GC_5K,reads_5K,ylim = c(0,500),cex = 0.5)
# In a-b line, intercept is a and slope is b
abline(a=reg_1$coefficients[1],b=reg_1$coefficients[2],col = 4, lw=3)
abline(a=reg_2$coefficients[1],b=reg_2$coefficients[2],col = 2, lw=3)

```

## The meaning of confidence intervals for the wrong model

Lets' compute approximate CIs for the parameters
```{r}
plot(GC_5K,reads_5K,ylim = c(0,500),cex = 0.5, rgb = c(0,0,0,0.5))

sum_reg_2 = summary(reg_2)
# also draw the +-2 SD intervals for the mean
lower_bd_a = sum_reg_2$coefficients[1,1]-2*sum_reg_2$coefficients[1,2]
upper_bd_a = sum_reg_2$coefficients[1,1]+2*sum_reg_2$coefficients[1,2]

lower_bd_b = sum_reg_2$coefficients[2,1]-2*sum_reg_2$coefficients[2,2]
upper_bd_b = sum_reg_2$coefficients[2,1]+2*sum_reg_2$coefficients[2,2]
abline(a=lower_bd_a,b=lower_bd_b,col = 4, lw=3,lt=2)
abline(a=upper_bd_a,b=upper_bd_b,col = 4, lw=3,lt=2)
```

A better interval uses also the covariance between the parameters. 

```{r}

plot(GC_5K,reads_5K,ylim = c(0,500),cex = 0.5, rgb = c(0,0,0,0.5))
conf_band = predict(reg_2, list(GC_5K = (0:4000)/5000), interval="confidence") 
lines((0:4000)/5000, conf_band[,2],col = 4,lt=3,lw=3)
lines((0:4000)/5000, conf_band[,3],col = 4,lt=3,lw=3)
```

But this is obviously wrong !! 

Reminder:

\[ b_1 = ((X'X)^{-1} X'Y)_{2},  X = (1,GC) \]

Then 
\[b_1 = \frac{cov(gc, coverage)}{var(gc)} \]

and is an estimator for 
\[\beta_1 = \frac{Cov(GC, Coverage)}{Var(GC)}.\]

# Part 2, non-parametric regression


Simulate data

```{r}
set.seed(100)
x = sort(c(runif(100,-5,5)))
y = sin(x) + 0.5*x + 0.03*x^2 + rnorm(100)
plot(x,y,cex=0.8)
# Plot true equation
lines(x,sin(x) + 0.5*x + 0.03*x^2)

plot_sim = function(x,y,lim_y = c(-4,3.5)){
  plot(x,y, ylim = lim_y)
  lines(x,sin(x) + 0.5*x + 0.03*x^2)
}

```



## A problem with polynomial data

```{r}
plot_sim(x,y, lim_y = c(-6,3))
cube_mod = lm(y~I(x^3)+I(x^2)+x)
lines(x, predict(cube_mod),lw=2,col=3)

# What happens when we change a few small values
y_corrupt = y; y_corrupt[1:5]=-6
cube_mod_corrupt = lm(y_corrupt~I(x^3)+I(x^2)+x)
points(x,y_corrupt,cex = 0.7,pch = 'x')
lines(x, predict(cube_mod_corrupt),lw=2,col=4)
```


## Polynomial in regions
Set up regions ("knots"):
```{r}
plot_sim(x,y)
abline(v=c(-2,2),lt=2)
grp = 1+(x>-2)+(x>2)
```

```{r}
knitr::kable(table(grp))
```

```{r}
lim.pts = c(-5,-2,2,5)
new = data.frame(x=c(seq(-5,-2.001,len=20),seq(-1.999,1.999,len=30),seq(2.001,5,len=20)))

plot_sim = function(x,y){
  plot(x,y)
  lines(x,sin(x) + 0.5*x + 0.03*x^2)
  abline(v=c(-2,2), lt=2)
}

```

Approximating by a piece-wise constant function
```{r}

pwc = by(y,grp,mean) # Mean for each group
plot_sim(x,y)
lines(c(-5,-2),c(pwc[1],pwc[1]),col="blue",lwd=2)
lines(c(-2,2),c(pwc[2],pwc[2]),col="blue",lwd=2)
lines(c(2,5),c(pwc[3],pwc[3]),col="blue",lwd=2)
#res = y-pwc[grp]
#plot(res[1:(length(y)-1)],res[2:length(y)])
```

Approximating by a piece-wise linear function
```{r}
plot_sim(x,y)
for (i in 1:3){
  x.i = x[grp==i]
  y.i = y[grp==i]
  lr.i = lm(y.i~x.i)$coefficients
  lines(lim.pts[c(i,i+1)],lim.pts[c(i,i+1)]*lr.i[2]+lr.i[1],col="blue",lwd=2)
}
```

A new version of getting piece-wise constant
```{r}
# pw constant - another look
plot_sim(x,y)
pwc_const = lm(y ~ I( (x< -2)) + I((x<2 & x>-2)) + I((x>2)))
summary(pwc_const)
lines(new$x,predict(pwc_const,new),lwd=2,col='blue')
```

A new version of getting piece-wise linear
```{r}
plot_sim(x,y)
pwc_lin = (lm(y ~ I(x*(x< -2)) + I((x<2 & x>-2)) + I((x>2)) + I(x*(x<2 & x>-2)) + I(x*(x>2))))
lines(new$x,predict(lm(y ~ I(x*(x< -2)) + I((x<2 & x>-2)) + I((x>2)) + I(x*(x<2 & x>-2)) + I(x*(x>2))),new),lwd=2,col='blue')
```

```{r}
plot(x, -x*(x< -2),col = 0)
lines(x, -x*(x< -2))
lines(x, x^2*(x< -2)/5, col = 2)
```

Region regression can be run on each region separately, though it makes it a bit harder to get residuals. 
