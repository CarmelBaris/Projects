---
title: "Task2"
date: Sys.Date()
output:
  html_document:
    code_folding: show
  pdf_document: default
editor_options:
  markdown:
    wrap: sentence
---

Team 6:
 - Shawn Yakir , 315714980, shawnyakir@gmail.com  
 - Sarah Levitz, 324673623, sarah.levitz@mail.huji.ac.il
 - Carmel Baris, 318455276, carmel.baris@mail.huji.ac.il


### Libraries used

```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
library('ggplot2') # fread
library('data.table') # Part A_Q1
library('tidyr')
library('dplyr')
```


### Paths and Data


```{r data_A, message=FALSE, warning=FALSE, include=FALSE}

#helper functions
source("C:/Users/user/Documents/Carmel/Projects/HUJI/Archive_HUJI/data-analysis-genomics/data/help_funcs_v3.R")

# Load the data (Group A)
reads_file = 'C:/Users/user/Documents/Carmel/Projects/HUJI/Archive_HUJI/data-analysis-genomics/data/TCGA-13-0723-01A_lib2_all_chr1.forward'
chr1_reads = fread(reads_file) 
colnames(chr1_reads) = c("Chrom","Loc","FragLen")

# Get read line of actual data
locations = chr1_reads$Loc
beg_region = 1
end_region = 1+2e07
reads_20K = getReadLine(locations, beg_region, end_region)
Nsingle = length(reads_20K)
Ssingle = sum(reads_20K)
```


## Introduction 

```{r}
# Calculate probabilities
obs_prob_singl <- Ssingle / Nsingle
exp_pois_upto5 = dpois(0:5, lambda)
exp_pois_plus5 = sum(dpois(6:max(reads_20K), lambda))
expected_prob = c(exp_pois_upto5, exp_pois_plus5)
expected_prob = round(expected_prob,2)

```


## Avg coverage and distance between reads
```{r}
last_read = as.numeric(chr1_reads[nrow(chr1_reads),"Loc"])

# {Average Coverage} = {Total Reads}/{Total Bases}
avg_coverage = nrow(chr1_reads)*(1 / last_read)



```

## Compare the expected distribution versus the actual distribution

```{r}
# produce many instances of uniform variables
set.seed(1)
nsamp = 100e+06
max_samp = nsamp * 10
unif_dat = runif(n = nsamp, 0,max_samp )
s_dat = sort(unif_dat)

# obtain Poisson by counting occurrences in equal-size intervals 
k = 1000
output_1K = help_funcs_v3::binReads(ceiling(s_dat), k)

```


### Observed distribution of the data (count)
```{r}

# Filter out values for 0-5 and aggregate 5+ as 5+
Nsingle = length(reads_20K)
line_filtered = ifelse(reads_20K > 5, 5, reads_20K)
obs_prob_singl = setNames(numeric(7), c(as.character(0:5), "5+"))
obs_prob_singl[as.character(0:5)] = table(factor(line_filtered, levels = 0:5))
obs_prob_singl["5+"] = sum(reads_20K > 5) / Nsingle
# obs_prob_singl = round(sum(reads_20K) / Nsingle, 2)
obs_prob_single = sum(reads_20K) / Nsingle # in region

lambda = mean(reads_20K)  # Mean of observed counts

# Calculate expected probabilites for 0-5 and aggregate the rest as 5+

# Calculate the expected distribution for a uniform Poisson model
exp_pois_upto5 = dpois(0:5, lambda)

exp_pois_plus5 = sum(dpois(6:max(reads_20K), lambda))

# join to merged probs object
expected_prob = c(exp_pois_upto5, exp_pois_plus5)

expected_prob = round(expected_prob, 2) 
names(expected_prob) = c(as.character(0:5), "5+") # add column names
obs_prob_singl
```



### Comparing the densities in a table:
```{r}


# Round the counts to 5 decimal places
obs_prob_singl_ = round(obs_prob_singl, 5)

expected_prob_ = round(expected_prob, 5)

# Create a data frame
prob_df_ = data.frame(
  pois_val_ = c("expected", "observed"),
  "0"  = c(expected_prob_[1], obs_prob_singl_[1]),
  "1"  = c(expected_prob_[2], obs_prob_singl_[2]),
  "2"  = c(expected_prob_[3], obs_prob_singl_[3]),
  "3"  = c(expected_prob_[4], obs_prob_singl_[4]),
  "4"  = c(expected_prob_[5], obs_prob_singl_[5]),
  "5"  = c(expected_prob_[6], obs_prob_singl_[6]),
  "5+" = c(expected_prob_[7], obs_prob_singl_[7])
  )

# Create a data frame rounded to 2 decimal points and display the result

obs_prob_singl = round(obs_prob_singl, 2)
expected_prob = round(expected_prob, 2)
prob_df = data.frame(
  pois_val = c("expected", "observed"),
  "0"  = c(expected_prob[1], obs_prob_singl[1]),
  "1"  = c(expected_prob[2], obs_prob_singl[2]),
  "2"  = c(expected_prob[3], obs_prob_singl[3]),
  "3"  = c(expected_prob[4], obs_prob_singl[4]),
  "4"  = c(expected_prob[5], obs_prob_singl[5]),
  "5"  = c(expected_prob[6], obs_prob_singl[6]),
  "5+" = c(expected_prob[7], obs_prob_singl[7])
  )

print(prob_df, right = TRUE)


```


Graphical comparison:
```{r}
# Create a data frame for the graphical comparison
comparison_df = data.frame(
  Count = names(obs_prob_singl),
  Observed = as.numeric(obs_prob_singl),
  Expected = as.numeric(expected_prob)
)

# Reshape data to long format using pivot_longer
comparison_df_long = comparison_df %>%
  pivot_longer(cols = c("Observed", "Expected"), names_to = "Type", values_to = "Value")

# Plot
ggplot(comparison_df_long, aes(x = Count, y = Value, fill = Type)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  scale_fill_manual(values = c("Observed" = "darkblue", "Expected" = "lightblue")) +
  labs(title = "Observed vs Expected Poisson Distribution Probabilities",
       y = "Probabilities",
       x = "Read Counts") +
  theme_minimal()


```



# Step 5: Numerical metric (Chi-square goodness-of-fit test)

```{r}
# Load required library
library(data.table)

# Load the data
reads_file = "/Users/alevi/Downloads/ATCGA-13-0723-01A_lib2_all_chr1.forward/TCGA-13-0723-01A_lib2_all_chr1.forward"
chr1_reads = fread(reads_file)
colnames(chr1_reads) = c("Chrom", "Loc", "FragLen")

# Define bin size
bin_size = 5000

# Calculate number of bins
total_length = max(chr1_reads$Loc)
num_bins = ceiling(total_length / bin_size)

# Create bins
bins = cut(chr1_reads$Loc, breaks=seq(0, total_length, by=bin_size), include.lowest=TRUE)

# Count fragments per bin
fragment_counts = table(bins)
fragment_counts = as.numeric(fragment_counts)

# Filter outliers
q1 = quantile(fragment_counts, 0.25)
q3 = quantile(fragment_counts, 0.75)
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
filtered_counts = fragment_counts[fragment_counts >= lower_bound & fragment_counts <= upper_bound]

# Estimate mean and variance
mean_count = mean(filtered_counts)
var_count = var(filtered_counts)

# Expected variance for Poisson distribution
lambda = mean_count
expected_variance = lambda

# Compute Poisson distribution values
x_vals = seq(0, max(filtered_counts), by=1)
poisson_vals = dpois(x_vals, lambda)

# Determine y-axis limit
max_density = max(hist(filtered_counts, breaks=30, plot=FALSE)$density)
max_poisson_density = max(poisson_vals)
y_max = max(max_density, max_poisson_density) * 1.1

# Plot histogram of the actual data
hist(filtered_counts, breaks=30, freq=FALSE, main="Fragment Counts per Bin", xlab="Fragment Count", ylab="Density", ylim=c(0, y_max))

# Overlay Poisson distribution
lines(x_vals, poisson_vals, col="red")

# Chi-squared test
observed_counts = hist(filtered_counts, breaks=30, plot=FALSE)$counts
expected_counts = dpois(0:(length(observed_counts)-1), lambda) * sum(observed_counts)

# Rescale expected counts to match observed counts
expected_counts = expected_counts / sum(expected_counts) * sum(observed_counts)

chisq_test = chisq.test(observed_counts, p=expected_counts, rescale.p=TRUE)

# Print test results
print(chisq_test)


```




## Short summary

Based on our analysis of the data, it seems that the reads are not distributed normally or uniformly.

