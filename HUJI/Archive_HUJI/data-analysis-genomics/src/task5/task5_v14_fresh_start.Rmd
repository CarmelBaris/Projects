---
title: "Lab Task 5"
author: "Group 6"
date: Sys.Date()
output:
  html_document:
    code_folding: show
editor_options: 
  markdown: 
    wrap: sentence
---

Group members:

- Shawn Yakir , 315714980, shawnyakir@gmail.com
- Sarah Levitz, 324673623, sarah.levitz@mail.huji.ac.il
- Carmel Baris, 318455276, carmel.baris@mail.huji.ac.il 


## SETUP
```{r setup}
# Clean the environment
rm(list = ls())

# Load required packages
library('data.table')
library('splines')

# Set working directory
# dir_name = "/Users/alevi/Downloads/"
dir_name = "C:/Users/user/Documents/Carmel/Projects/HUJI/Archive_HUJI/data-analysis-genomics/data/"
# dir_name = "\\Users\\shawn\\Desktop\\"

# Load helper functions
helpers_file = file.path(dir_name, "help_funcs.R")
source(helpers_file)
helpers = list(
    loadRData = loadRData,
    fitMet = fitMetrics,
    binBases = binBases,
    binReads = binReads,
    check_ratio = check_proportions,
    check_overlap = check_overlaps,
    check_size = check_data_size,
    fit_bspline = fit_bspline,
    plot_bspline = plot_bspline_results
    )

# Load data
bases_file = file.path(dir_name, "chr1_line.rda")
chr1_bases = helpers$loadRData(bases_file)
reads_file_all = file.path(dir_name, "TCGA-13-0723-01A_lib2_all_chr1.forward")
chr1_reads = data.table::fread(reads_file_all)
colnames(chr1_reads) = c("Chrom", "Loc", "FragLen")

# Preprocess data
nreads = nrow(chr1_reads)
myLocations = as.numeric(chr1_reads$Loc)
last_read = myLocations[nreads]
binsize = 20000
bases_20K = helpers$binBases(chr1_bases, last_read, binsize)
GC_20K = bases_20K[, 3] + bases_20K[, 4]
GC_20K = GC_20K / binsize
reads_20K = helpers$binReads(myLocations, binsize, last_read)
```


## OUTLIERS
```{r outliers}
# Defining logical vectors for filtering out bins (observations)

# outliers of gc percent per bin
zeros_GC = GC_20K == 0 #bins w/o GC content
small_GC = GC_20K <= 0.3 & GC_20K > 0 #low-GC content bins (<30%)
valid_GC = !(zeros_GC | small_GC) #bins w/ at least 30% GC content

# identify outliers of reads count per bin
z_scores = (reads_20K - mean(reads_20K)) / sd(reads_20K)
xtreme_reads = abs(z_scores) > 2

# create new variables without outliers
outliers = (zeros_GC | small_GC | xtreme_reads)
reads_20K_clean = reads_20K[!outliers]
GC_20K_clean = GC_20K[!outliers]

```



## BENCHMARK (FITTING BSPLINE REGRESSION WITH ALL DATA)
```{r fit}
plot(GC_20K, reads_20K, pch = 16, cex = 0.5, col = rgb(0, 0, 0, 0.7),
     xlab = "GC Content (cleaned)", ylab = "Read Counts (cleaned)",
     xlim = c(min(GC_20K_clean),max(GC_20K_clean)),
     ylim = c(min(reads_20K_clean),max(reads_20K_clean)),
     main = "Piecewise Polynomial Regression")


# Fit the regression models
knots_bs = c(0.415, 0.45, 0.54)
lknot = min(GC_20K)
rknot = max(GC_20K)
reg_bs = lm(subset = !outliers,
             formula = reads_20K ~ bs(GC_20K, degree = 3, 
                                      knots = knots_bs, 
                                      Boundary.knots = c(lknot, rknot)))

# Create new data for prediction
new_data <- data.frame(GC_20K = seq(min(GC_20K_clean), max(GC_20K_clean), length.out = 1000))
new_data$predicted <- predict(reg_bs, newdata = new_data)

# Plot the B-spline regression line
lines(new_data$GC_20K, new_data$predicted, col = "blue", lwd = 2)

# Add vertical lines to indicate breakpoints
abline(v = knots_bs, lty = 2, col = "darkgray", lwd = 1.5)

# Add legend
legend("topleft", legend = "B-spline Regression", col = "blue", 
       bty = "n",lwd = 2, cex = 0.7)

```


## QUESTION 1A 
```{r split}
# Set seed for reproducibility
set.seed(100)

# Division 1: Data into 20 parts and randomly assigning 14 parts to train
n_bins <- length(reads_20K)
bins_per_group <- n_bins %/% 20  # Integer division to get bins per group
extra_bins <- n_bins %% 20       # Remainder to ensure equal-sized groups

# Distribute extra bins (if any) across the groups
group_sizes <- rep(bins_per_group, 20)
if (extra_bins > 0) {
  group_sizes[1:extra_bins] <- group_sizes[1:extra_bins] + 1
}

# Create the 20 groups
groups <- split(1:n_bins, rep(1:20, group_sizes))

# Randomly select 14 groups for training and 6 for testing
train_div1_indices <- sample(1:20, 14)
test_div1_indices <- setdiff(1:20, train_div1_indices)

# Create logical vectors for Division 1
train_div1 = rep(FALSE, n_bins)
test_div1 = rep(FALSE, n_bins)

train_div1_vec[unlist(groups[train_div1_indices])] <- TRUE
test_div1_vec[unlist(groups[test_div1_indices])] <- TRUE

# Division 2: Completely random division (70% train, 30% test)
n_samples <- length(reads_20K)
train_indices_div2 <- sample(1:n_samples, 0.7 * n_samples)
test_indices_div2 <- setdiff(1:n_samples, train_indices_div2)

# Create logical vectors for Division 2
train_div2 = rep(FALSE, n_samples)
test_div2 = rep(FALSE, n_samples)

train_div2_vec[train_indices_div2] <- TRUE
test_div2_vec[test_indices_div2] <- TRUE

# Division 3: Regular division (first 70% train, last 30% test)
train_div3_indices <- 1:floor(0.7 * n_samples)
test_indices_div3 <- (floor(0.7 * n_samples) + 1):n_samples

# Create logical vectors for Division 3
train_div3 = rep(FALSE, n_samples)
test_div3 = rep(FALSE, n_samples)

train_div3[train_div3_indices] <- TRUE
test_div3[test_indices_div3] <- TRUE

```




## Sanity Partitioning

```{r split_qa}
#+++++++++++ Check Proportions
# Division 1
props_div1 = helpers$check_ratio(reads_20K_clean[train_div1], reads_20K_clean[test_div1], 14/20)
cat("Div1 props\n", 
    "Test proportion:", props_div1$test_ratio, "\n",
    "Train proportion:", props_div1$train_ratio, "\n",
    "Expected proportion of train:", props_div1$expected_train_ratio,
     "\n\n"
    )

# Division 2
props_div2 = helpers$check_ratio(train_div2, test_div2, 0.7)
cat("Div2 props\n", 
    "Test proportion:", props_div2$test_ratio, "\n",
    "Train proportion:", props_div2$train_ratio, "\n",
    "Expected proportion of train:", props_div2$expected_train_ratio,
     "\n\n"
    )

# Division 3
props_div3 = helpers$check_ratio(train_div3, test_div3, 0.7)
cat("Div3 props\n", 
    "Test proportion:", props_div3$test_ratio, "\n",
    "Train proportion:", props_div3$train_ratio, "\n",
    "Expected proportion of train:", props_div3$expected_train_ratio,
     "\n\n"
    )

#+++++++++++ Check for Overlaps
cat("\n OVERLAP TEST \n\n")

# Division 1
overlaps_div1 = helpers$check_overlap(train_div1_indices, test_div1_indices)
paste0("Has Div1 passed overlap test? ", overlaps_div1)

# Division 2
overlaps_div2 = helpers$check_overlap(train_indices_div2, test_indices_div2)
paste0("Has Div2 passed overlap test? ", overlaps_div2)

# Division 3
overlaps_div3 = helpers$check_overlap(train_div3_indices, test_indices_div3)
paste0("Has Div3 passed overlap test? ", overlaps_div3)

#+++++++++++ Check Data Integrity

cat("\n SIZE TEST\n\n")

# Division 1
size_div1 = helpers$check_size(train_div1, test_div1, reads_20K)
paste0("Has Div1 passed size test? ", size_div1)

# Division 2
size_div2 = helpers$check_size(train_div2, test_div2, reads_20K)
paste0("Has Div2 passed size test? ", size_div2)

# Division 3
size_div3 = helpers$check_size(train_div3, test_div3, reads_20K)
paste0("Has Div3 passed size test? ", size_div3)



```


## QUESTION 1B: COMPARING FIT

```{r train_fit}
train_data_div1 <- data.frame(GC_20K = GC_20K_clean[train_div1_vec], reads_20K = reads_20K_clean[train_div1_vec])
train_data_div2 <- data.frame(GC_20K = GC_20K_clean[train_div2_vec], reads_20K = reads_20K_clean[train_div2_vec])
train_data_div3 <- data.frame(GC_20K = GC_20K_clean[train_div3_vec], reads_20K = reads_20K_clean[train_div3_vec])

# Define knots for the B-spline
knots_bs <- c(0.415, 0.45, 0.54)

# Fit the B-spline model using the train data for each division
reg_bs_train_div1 <- lm(reads_20K ~ bs(GC_20K, knots = knots_bs, degree = 3, Boundary.knots = c(min(GC_20K_clean), max(GC_20K_clean))), data = train_data_div1)
reg_bs_train_div2 <- lm(reads_20K ~ bs(GC_20K, knots = knots_bs, degree = 3, Boundary.knots = c(min(GC_20K_clean), max(GC_20K_clean))), data = train_data_div2)
reg_bs_train_div3 <- lm(reads_20K ~ bs(GC_20K, knots = knots_bs, degree = 3, Boundary.knots = c(min(GC_20K_clean), max(GC_20K_clean))), data = train_data_div3)

# Create new data for prediction
new_data <- data.frame(GC_20K = seq(min(GC_20K_clean), max(GC_20K_clean), length.out = 1000))

# Predict for each division
predicted_div1 <- predict(reg_bs_train_div1, newdata = new_data)
predicted_div2 <- predict(reg_bs_train_div2, newdata = new_data)
predicted_div3 <- predict(reg_bs_train_div3, newdata = new_data)

```


## COMPARING REGRESSION LINES

```{r train_plots}
# Plot the data and the B-spline regression lines for each division
plot(GC_20K_clean, reads_20K_clean, pch = 16, cex = 0.5, col = rgb(0, 0, 0, 0.7),
     xlab = "GC Content (cleaned)", ylab = "Read Counts (cleaned)",
     xlim = c(min(GC_20K_clean), max(GC_20K_clean)),
     ylim = c(0, 3500),
     main = "B-Spline Regression on Train Data")

# Plot the B-spline regression lines for each division
lines(new_data$GC_20K, predicted_div1, col = "blue", lwd = 2)
lines(new_data$GC_20K, predicted_div2, col = "red", lwd = 2)
lines(new_data$GC_20K, predicted_div3, col = "green", lwd = 2)

# Add vertical lines to indicate breakpoints
abline(v = knots_bs, lty = 2, col = "darkgray", lwd = 1.5)

# Add legend
legend("topleft", legend = c("70% Random Regions (Div 1)", "70% Random Bins (Div 2)", "70% First Bins on Left (Div 3)"), col = c("blue", "red", "green"), lwd = 2, cex = 0.75, bty = "n")

```

## QUESTION 1C+1D
# Calculate metrics for train and test data in each division
```{r split_metrics}


# Subset test data for each division
test_data_div1 <- data.frame(GC_20K = GC_20K_clean[test_div1_vec], reads_20K = reads_20K_clean[test_div1_vec])
test_data_div2 <- data.frame(GC_20K = GC_20K_clean[test_div2_vec], reads_20K = reads_20K_clean[test_div2_vec])
test_data_div3 <- data.frame(GC_20K = GC_20K_clean[test_div3_vec], reads_20K = reads_20K_clean[test_div3_vec])

# Predict for test data in each division
test_data_div1$predicted <- predict(reg_bs_train_div1, newdata = test_data_div1)
test_data_div2$predicted <- predict(reg_bs_train_div2, newdata = test_data_div2)
test_data_div3$predicted <- predict(reg_bs_train_div3, newdata = test_data_div3)

```



## Calculate metrics for train and test data in each division


```{r calculate_metrics}
# Define a function to calculate R-squared, MAE, and MSE
calculate_metrics <- function(actual, predicted) {
  # Remove NA values from actual and predicted
  actual <- actual[!is.na(actual)]
  predicted <- predicted[!is.na(predicted)]
  
  if (length(actual) != length(predicted)) {
    stop("Length of 'actual' and 'predicted' must be the same.")
  }
  
  n <- length(actual)
  
  if (n == 0) {
    stop("No valid data points to calculate metrics.")
  }
  
  # Calculate R-squared
  mean_actual <- mean(actual)
  ss_total <- sum((actual - mean_actual)^2)
  ss_residual <- sum((actual - predicted)^2)
  r_squared <- 1 - ss_residual / ss_total
  
  # Calculate MAE and MSE
  mae <- mean(abs(actual - predicted))
  mse <- mean((actual - predicted)^2)
  
  return(list(R_squared = r_squared, MAE = mae, MSE = mse))
}
# Calculate metrics for train and test data in each division
metrics_train_div1 <- calculate_metrics(train_data_div1$reads_20K,
                                        predict(reg_bs_train_div1, 
                                                newdata = train_data_div1))

metrics_test_div1 <- calculate_metrics(test_data_div1$reads_20K, 
                                       test_data_div1$predicted)

metrics_train_div2 <- calculate_metrics(train_data_div2$reads_20K, 
                                        predict(reg_bs_train_div2, 
                                                newdata = train_data_div2))

metrics_test_div2 <- calculate_metrics(test_data_div2$reads_20K,
                                       test_data_div2$predicted)

metrics_train_div3 <- calculate_metrics(train_data_div3$reads_20K, 
                                        predict(reg_bs_train_div3, 
                                                newdata = train_data_div3))

metrics_test_div3 <- calculate_metrics(test_data_div3$reads_20K, 
                                       test_data_div3$predicted)

# Print metrics
print("Division 1 Train Metrics:")
print(metrics_train_div1)
print("Division 1 Test Metrics:")
print(metrics_test_div1)

print("Division 2 Train Metrics:")
print(metrics_train_div2)
print("Division 2 Test Metrics:")
print(metrics_test_div2)

print("Division 3 Train Metrics:")
print(metrics_train_div3)
print("Division 3 Test Metrics:")
print(metrics_test_div3)

```




```{r split_plots}
# Create plots for each division separately

# Division 1
par(mfrow = c(1, 1))  # Reset to single plot
plot(train_data_div1$GC_20K, train_data_div1$reads_20K, pch = 16, cex = 0.5, col = rgb(0, 0, 1, 0.5),
     xlab = "GC Content (cleaned)", ylab = "Read Counts (cleaned)",
     xlim = c(min(GC_20K_clean), max(GC_20K_clean)),
     ylim = c(0, max(reads_20K_clean)),
     main = "Division 1: Piecewise Polynomial Regression")

lines(new_data$GC_20K, predicted_div1, col = "blue", lwd = 2)
points(test_data_div1$GC_20K, test_data_div1$reads_20K, pch = 16, cex = 0.5, col = rgb(1, 0, 0, 0.5))
points(test_data_div1$GC_20K, test_data_div1$predicted, pch = 16, cex = 0.5, col = rgb(0, 1, 0, 0.5))

legend("topleft", legend = c("Train Data", "Test Data Actual", "Test Data Predicted", "B-spline Fit"),
       col = c(rgb(0, 0, 1, 0.5), rgb(1, 0, 0, 0.5), rgb(0, 1, 0, 0.5), "blue"), lwd = 2, pch = 16, cex = 0.7)

# Division 2
plot(train_data_div2$GC_20K, train_data_div2$reads_20K, pch = 16, cex = 0.5, col = rgb(0, 0, 1, 0.5),
     xlab = "GC Content (cleaned)", ylab = "Read Counts (cleaned)",
     xlim = c(min(GC_20K_clean), max(GC_20K_clean)),
     ylim = c(0, max(reads_20K_clean)),
     main = "Division 2: Piecewise Polynomial Regression")

lines(new_data$GC_20K, predicted_div2, col = "red", lwd = 2)
points(test_data_div2$GC_20K, test_data_div2$reads_20K, pch = 16, cex = 0.5, col = rgb(1, 0, 0, 0.5))
points(test_data_div2$GC_20K, test_data_div2$predicted, pch = 16, cex = 0.5, col = rgb(0, 1, 0, 0.5))

legend("topleft", legend = c("Train Data", "Test Data Actual", "Test Data Predicted", "B-spline Fit"),
       col = c(rgb(0, 0, 1, 0.5), rgb(1, 0, 0, 0.5), rgb(0, 1, 0, 0.5), "red"), lwd = 2, pch = 16, cex = 0.7)

# Division 3
plot(train_data_div3$GC_20K, train_data_div3$reads_20K, pch = 16, cex = 0.5, col = rgb(0, 0, 1, 0.5),
     xlab = "GC Content (cleaned)", ylab = "Read Counts (cleaned)",
     xlim = c(min(GC_20K_clean), max(GC_20K_clean)),
     ylim = c(0, max(reads_20K_clean)),
     main = "Division 3: Piecewise Polynomial Regression")

lines(new_data$GC_20K, predicted_div3, col = "green", lwd = 2)
points(test_data_div3$GC_20K, test_data_div3$reads_20K, pch = 16, cex = 0.5, col = rgb(1, 0, 0, 0.5))
points(test_data_div3$GC_20K, test_data_div3$predicted, pch = 16, cex = 0.5, col = rgb(0, 1, 0, 0.5))

legend("topleft", legend = c("Train Data", "Test Data Actual", "Test Data Predicted", "B-spline Fit"),
       col = c(rgb(0, 0, 1, 0.5), rgb(1, 0, 0, 0.5), rgb(0, 1, 0, 0.5), "green"), lwd = 2, pch = 16, cex = 0.7)


```


## QUESTION 2: 
Train 
First plot against GC
Second plot against prediction

```{r}
# Verify the dimensions of train_data_div3
print(dim(train_data_div3))

# Ensure there are no NA or infinite values in the relevant columns
train_data_div3 <- train_data_div3[complete.cases(train_data_div3$GC_20K, train_data_div3$reads_20K), ]
train_data_div3 <- train_data_div3[is.finite(train_data_div3$GC_20K) & is.finite(train_data_div3$reads_20K), ]

# Fit the B-spline model again with the cleaned train data
reg_bs_train_div3 <- lm(reads_20K ~ bs(GC_20K, knots = knots_bs, degree = 3, Boundary.knots = c(min(GC_20K_clean), max(GC_20K_clean))), data = train_data_div3)

# Predict for the cleaned train data
train_data_div3$predicted <- predict(reg_bs_train_div3, newdata = train_data_div3)

# Calculate residuals for the train data
train_data_div3$residuals <- train_data_div3$reads_20K - train_data_div3$predicted

# Calculate metrics for train data
metrics_train_div3 <- calculate_metrics(train_data_div3$reads_20K, train_data_div3$predicted)

# Print the metrics for train data
print(metrics_train_div3)

# Plot the residuals against GC content for the train data
plot(train_data_div3$GC_20K, train_data_div3$residuals, 
     pch = 16, cex = 0.5, col = rgb(1, 0, 0, 0.5),
     xlab = "GC Content (cleaned)", ylab = "Residuals",
     main = "Residuals vs GC Content (Train Data)")

# Plot the residuals against the predicted coverage for the train data
plot(train_data_div3$predicted, train_data_div3$residuals, 
     pch = 16, cex = 0.5, col = rgb(0, 0, 1, 0.5),
     xlab = "Predicted Coverage", ylab = "Residuals",
     main = "Residuals vs Predicted Coverage (Train Data)")
```


Test 
First plot against GC
Second plot against prediction
```{r}
# Ensure test_data_div3 has predicted values
test_data_div3$predicted <- predict(reg_bs_train_div3, newdata = test_data_div3)

# Calculate residuals for test data
test_data_div3$residuals <- test_data_div3$reads_20K - test_data_div3$predicted

# Plot residuals against GC content for test data
plot(test_data_div3$GC_20K, test_data_div3$residuals, 
     pch = 16, cex = 0.5, col = rgb(1, 0, 0, 0.5),
     xlab = "GC Content (cleaned)", ylab = "Residuals",
     main = "Residuals vs GC Content (Test Data)")

# Plot residuals against predicted coverage for test data
plot(test_data_div3$predicted, test_data_div3$residuals, 
     pch = 16, cex = 0.5, col = rgb(0, 0, 1, 0.5),
     xlab = "Predicted Coverage", ylab = "Residuals",
     main = "Residuals vs Predicted Coverage (Test Data)")
```





Analysis of Train Data:
Precision, the model’s ability to describe the main trend lines of the existing data, that the training set consists of.
Division 1 vs. Division 2:
Both divisions have similar $R^2$ values (~0.49), indicating similar explanatory power on the training data.
Division 2 has slightly lower MAE and MSE, suggesting a marginally better fit.
Division 1 vs. Division 3:
Division 3 has a lower $R^2$ (0.4308802) compared to Division 1 (0.4887023), indicating that it explains less variance in the training data.
Division 3 has higher MAE and MSE, indicating larger prediction errors on the training data.
Division 2 vs. Division 3:
Division 2 performs better than Division 3 in terms of $R^2$, MAE, and MSE, indicating a better fit to the training data.
Analysis of Test Data:
Prediction, the model’s ability to generalize the trend line to new data points that weren’t included in the training.
Division 1 vs. Division 2:
Division 2 has a slightly higher $R^2$ and lower MSE on the test data, suggesting it generalizes slightly better than Division 1.
However, Division 1 has a lower MAE, indicating smaller average errors.
Division 1 vs. Division 3:
Division 3 has a significantly higher $R^2$ on the test data (0.6970413), indicating much better explanatory power.
Division 3 also has lower MAE and MSE, suggesting better predictive accuracy and smaller errors on the test data.
Division 2 vs. Division 3:
Division 3 outperforms Division 2 in all test data metrics, with higher $R^2$ and lower MAE and MSE.
Differences and Potential Causes:
Training Data Success:
The differences in success on the training data are relatively minor between Division 1 and Division 2, with Division 2 being slightly better.
Division 3 has the least success on the training data, likely due to the regular division approach, which may not capture the variability in the data as effectively as the other methods.
Test Data Success:
Division 3 shows the highest success on the test data, indicating that it generalizes better than the other divisions.
The success of Division 3 on the test data could be due to its regular division approach, which ensures that the model is trained on a consistent 70% of the data and tested on a consistent 30%. This method may help the model capture the overall trend better, resulting in improved generalization.
Potential Causes for Differences:
Division Approach: The method used to partition the data impacts how well the model learns and generalizes. Random divisions (Divisions 1 and 2) might lead to more variability and potential overfitting.
Data Homogeneity: Regular division (Division 3) ensures that the training and test sets are more homogeneous, leading to better generalization.
Data Representation: Ensuring a balanced data representation in both the train and the test sets can improve model performance on unseen data.
Conclusion:
Best for Train Data: Division 2 marginally performs the best for training data.
Best for Test Data: Division 3 performs the best for test data, indicating the best generalization.
Overall: Division 3 is recommended as it provides a good balance, achieving better generalization to unseen data, which is crucial for practical applications.

## QUESTION 3: 



