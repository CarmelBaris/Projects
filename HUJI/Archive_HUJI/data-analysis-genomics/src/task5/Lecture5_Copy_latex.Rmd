---
title: "Lecture5"
author: "Yuval Benjamini"
date: "11 November 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We simulate the data from the wrong model, but try to 
approximate it using polynomials.


# Spline regression

## Broken regression as linear model


## The problem?

Non continuous around the knots

# The solution is assure continuity

Build broken stick functions:
$$f(x,t) = (x-t) \,\text{  if  } \, x>t, \quad  0 \text{ otherwise }$$\
$$\hat{y} = b_0 + b_1\cdot x + b_2 f(x, -2) +  b_3 f(x, 2)$$


# What is the linear basis?


# Regression splines (cubic splines)

In regression splines we want not only continuity, but also continuous
second derivative. Build broken stick functions:

-   $f_1(x,t) = (x-t)$ if $x>t$, $0$ otherwise
-   $f_2(x,t) = (x-t)^2$ if $x>t$, $0$ otherwise
-   $f_3(x,t) = (x-t)^3$ if $x>t$, $0$ otherwise

$$\hat{y} = b_0 + b_1\cdot x + b_2\cdot x^2 + b_3 \cdot x^3 + f_3(x,-2) +  f_3(x,2)$$

Bspline has a similar idea, but each "predictor" is locally defined.
These are the linear basis functions:


These are the Cubic Basis spline functions


Here is an example of the regression results

This is the linear splines regression: 

This is the cubic splines regression:


## Using the linear model to predict an external set: 

In general, when fitting a linear model to transformed data, 
we always have two options:

Prepare the variable Or let lm compute by itself. 

In some cases, R already has linear bases coded that we 
can access ahead of time. Examples include `poly` and bsplines `bs()`


In either case, the `lm` codes the names of the variables. 

When we try to predict for a new dataset, it looks for 
the same variable names. 

Let's run polynom regression for d = 1,...,4
For drawing lines, it's important that the x's be sorted.

As we increase the degree, the errors go down. 
Remember, we are minimizing the fit within our function space. If function space increases, observed error will go down. 

However, for *independent* test data, we should not expect to see the error decay if we fit ourselves to the training noise. 

On a technical point, for the `predict` function to work on new data, the names of the variables must be identical. Then, any computations on the formula will be carried out on the new data.


Check how well the regression lines work on the test data, 
first by plotting

Then by looking at r-mspe:


Can we do too much? See d=17

Here RMSPE shows the overfit


## What happens if we take test data differently?

## d=3
