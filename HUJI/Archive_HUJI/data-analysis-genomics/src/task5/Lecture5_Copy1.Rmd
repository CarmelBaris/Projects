---
title: "Lecture5"
author: "Yuval Benjamini"
date: "11 November 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We simulate the data from the wrong model, but try to 
approximate it using polynomials.

```{r}
# simulate data for polynomial regression
set.seed(100)
x = sort(runif(100,-5,5))
y = sin(x) + 0.5*x + 0.03*x^2 + rnorm(100)

# test data

plot(x,y)
# Plot true equation
lines(x,sin(x) + 0.5*x + 0.03*x^2)

```

# Spline regression

## Broken regression as linear model

Set up regions ("knots"):

```{r}
plot_sim = function(x,y){
  plot(x,y)
  lines(x,sin(x) + 0.5*x + 0.03*x^2)
  abline(v=c(-2,2), lt=2)
}

grp = 1+(x>-2)+(x>2)
table(grp)
lim.pts = c(-5,-2,2,5)
new = data.frame(x=c(seq(-5,-2.001,len=20),seq(-1.999,1.999,len=30),seq(2.001,5,len=20)))


```

A new version of getting piece-wise constant

```{r}
# pw constant - another look
plot_sim(x,y)
pwc_const = lm(y ~ I( 1 * (x < -2) ) + I( 1 *((-2 <= x) & (x< 2)) ) + I(1 * ( 2 <= x) ) + 0)  
summary(pwc_const)
lines(new$x,predict(pwc_const,new),lwd=2,col='blue')
```

A new version of getting piece-wise linear

```{r}
plot_sim(x,y)
pwc_lin = summary(lm(y ~ I(1 * (x< -2)) + I(x* (x< -2)) + 
                       I(1 * (x<2 & x>-2)) +  I(x*(x<2 & x>-2))+ 
                       I(1*(x>2))  + I(x*(x>2))+0))
mod_broken = lm(y ~ I(1 * (x< -2)) + I(x* (x< -2)) + 
                       I(1 * (x<2 & x>-2)) +  I(x*(x<2 & x>-2))+ 
                       I(1*(x>2))  + I(x*(x>2))+0)
                       
lines(new$x,predict(mod_broken,new),lwd=2,col='blue')
```

## The problem?

Non continuous around the knots

# The solution is assure continuity

Build broken stick functions:
$$f(x,t) = (x-t) \,\text{  if  } \, x>t, \quad  0 \text{ otherwise }$$\
$$\hat{y} = b_0 + b_1\cdot x + b_2 f(x, -2) +  b_3 f(x, 2)$$

```{r}
# pw linear - cont.
plot_sim(x,y)
cont_lin = lm(y ~ x + I((x-(-2))*(x> -2)) + I((x-2)*(x>2)) )
summary(cont_lin)
lines(new$x,predict(cont_lin,new),lwd=2,col='blue')

```

# What is the linear basis?

```{r}
plot(x, x, col = 0)
lines(x,rep(1,length(x)), col = 1,lw=2)
lines(x,x, col = 2,lw=2)
lines(x,(x-(-2)) *(x>-2), col = 3,lw=2)
lines(x,(x-(2)) *(x>2), col = 4,lw=2)
title("Spline basis deg=1")
```

# Regression splines (cubic splines)

In regression splines we want not only continuity, but also continuous
second derivative. Build broken stick functions:

-   $f_1(x,t) = (x-t)$ if $x>t$, $0$ otherwise
-   $f_2(x,t) = (x-t)^2$ if $x>t$, $0$ otherwise
-   $f_3(x,t) = (x-t)^3$ if $x>t$, $0$ otherwise

$$\hat{y} = b_0 + b_1\cdot x + b_2\cdot x^2 + b_3 \cdot x^3 + f_3(x,-2) +  f_3(x,2)$$

```{r}

plot_sim(x,y)

xx = x^2
xxx = x^3
xxx2 = (x-(-2))^3*(x> -2)
xxx3 = (x-2)^3*(x>2)
lm(y ~ x+xx+xxx+xxx2+xxx3)

reg_spline = lm(y ~ x + I(x^2) + I(x^3) + I((x-(-2))^3*(x> -2)) + I((x-2)^3*(x>2)) )

summary(reg_spline)
lines(new$x,predict(reg_spline,new),lwd=2,col='green')
```

Bspline has a similar idea, but each "predictor" is locally defined.
These are the linear basis functions:

```{r}
library('splines')

spls = bs(x,knots = c(-2,2), degree=1)
plot(x, spls[,1],col=0,main = "Bspline basis functions")
lines(x, spls[,1],col=1)
lines(x, spls[,2],col=2)
lines(x, spls[,3],col=3)
```

These are the Cubic Basis spline functions

```{r}
spls = bs(x,knots = c(-2,2), degree=3)
plot(x, spls[,1],col=0,main = "Bspline basis functions")
lines(x, spls[,1],col=1)
lines(x, spls[,2],col=2)
lines(x, spls[,3],col=3)
lines(x, spls[,4],col=4)
lines(x, spls[,5],col=4)
```

Here is an example of the regression results

This is the linear splines regression: 

```{r}
bspline_mod = lm(y~bs(x,knots = c(-2,2), degree=1,Boundary.knots = c(min(x), max(x))))
plot(x, y, main = "B-Spline Regression", xlab = "x", ylab = "y")
new_dots = data.frame(x = seq(min(x), max(x), length.out = 100))
preds = predict(bspline_mod, new_dots)
lines(new_dots$x, preds, col = 2)
# lines(new$x, preds, col = 4)
```

This is the cubic splines regression:

```{r}
bspline_mod = lm(y~bs(x,knots = c(-2,2), degree=3))
preds = predict(bspline_mod, new)
plot_sim(x,y)
lines(new$x, preds,col=4)

```

## Using the linear model to predict an external set: 

In general, when fitting a linear model to transformed data, 
we always have two options:

Prepare the variable
```{r}
x2 = x^2
lm(y~x+x2)
```

```{r}
# Or let lm compute by itself. 
lm(y~x+I(x^2))
```

In some cases, R already has linear bases coded that we 
can access ahead of time. Examples include `poly`
```{r}
head(poly(x, degree = 2))
```
and bsplines `bs()`

```{r}
library('splines')
head(bs(x, knots = c(-2,2), degree = 2))
```

In either case, the `lm` codes the names of the variables. 

When we try to predict for a new dataset, it looks for 
the same variable names. 

Let's run polynom regression for d = 1,...,4
```{r}
# For drawing lines, it's important that the x's be sorted.
draw_line = list(x = seq(-5,5,l = 70))

# polynomial
# d=1
lm_1 = lm(y~x)
summary(lm_1)
plot(x,y)
lines(x,sin(x) + 0.5*x + 0.03*x^2)
lines(draw_line$x,predict(lm_1,draw_line),lwd=2,col='blue')
```
As we increase the degree, the errors go down. 
Remember, we are minimizing the fit within our function space. If function space increases, observed error will go down. 

## d=4
```{r}

lm_4 = lm(y~poly(x,4))
summary(lm_4)
plot(x,y)
lines(x,sin(x) + 0.5*x + 0.03*x^2)
lines(draw_line$x,predict(lm_4,draw_line),lwd=2,col='gray')
lines(draw_line$x,predict(lm_1,draw_line),lwd=2,col='blue')

```

However, for *independent* test data, we should not expect to see the error decay if we fit ourselves to the training noise. 

```{r}
set.seed(101)
xtest = runif(100,-5,5)
ytest = sin(xtest) + 0.5*xtest + 0.03*xtest^2 + rnorm(100)
# important for prediction that test data will have the same variable name
test_dat = data.frame(x=xtest)
```

On a technical point, for the `predict` function to 
work on new data, the names of the variables must be identical. Then, any computations on the formula will be carried out on the new data.


Check how well the regression lines work on the test data, 
first by plotting
```{r}
# Check on test
par(mfcol = c(2,1), mar = c(1,1,1,1))
plot(x,y,ylim = c(-4,4))
#lines(x,sin(x) + 0.5*x + 0.03*x^2)
lines(draw_line$x,predict(lm_4,draw_line),lwd=2,col='gray')

plot(xtest,ytest,ylim = c(-4,4))
#lines(x,sin(x) + 0.5*x + 0.03*x^2)
lines(draw_line$x,predict(lm_4,draw_line),lwd=2,col='gray')

```

Then by looking at r-mspe:

```{r}
# Check on test
rmspe_tr = sqrt(mean((y-predict(lm_4,list(x = x)))^2))
rmspe_test = sqrt(mean((ytest-predict(lm_4,test_dat))^2))

print(sprintf( "RMSPEs d=4: Train %.3f Test %.3f",rmspe_tr,rmspe_test))
```

Can we do too much? See d=17
```{r}
# d=17
lm_17 = lm(y~poly(x, 17) )
plot(x,y)
lines(x,sin(x) + 0.5*x + 0.03*x^2)
lines(draw_line$x,predict(lm_17,draw_line),lwd=2,col='gray')
```

Here RMSPE shows the overfit

```{r}
# Check on test
rmspe_tr = sqrt(mean((y-predict(lm_17,list(x = x)))^2))
rmspe_test = sqrt(mean((ytest-predict(lm_17,test_dat))^2))

print(sprintf( "RMSPEs d=17: Train %.3f Test %.3f",rmspe_tr,rmspe_test))
```

## What happens if we take test data differently?

## d=3
```{r}
set.seed(200)
new_train = list(x = sort(runif(100,-5,0)))
new_train$y = sin(new_train$x) + 0.5*new_train$x + 0.03*new_train$x^2 + rnorm(100)

new_test = list(x = sort(runif(100,0,5)))
new_test$y = sin(new_test$x) + 0.5*new_test$x + 0.03*new_test$x^2 + rnorm(100)
    
lm_3 = lm(y~poly(x,3))
lm_3b = lm(y~poly(x,3), data = new_train)
plot(new_train$x,new_train$y, xlim = c(-5,5), ylim= c(-4,4))
points(new_test$x,new_test$y, col = 2)
lines(x,sin(x) + 0.5*x + 0.03*x^2)
lines(draw_line$x,predict(lm_3,draw_line),lwd=2,col='green')
lines(draw_line$x,predict(lm_3b,draw_line),lwd=2,col='blue')

```


