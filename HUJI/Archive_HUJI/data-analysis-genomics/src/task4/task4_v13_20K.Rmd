---
title: "Lab Task 4: Approx. Regression w/ Piecewise Polynomial Model"
author: "Group 6"
date: Sys.Date()
output:
  html_document:
    code_folding: show
editor_options: 
  markdown: 
    wrap: sentence
---

Group members:

- Shawn Yakir , 315714980, shawnyakir@gmail.com
- Sarah Levitz, 324673623, sarah.levitz@mail.huji.ac.il
- Carmel Baris, 318455276, carmel.baris@mail.huji.ac.il 


# Libraries
```{r library, message=FALSE, warning=FALSE}
library(segmented)
library(data.table)
library(caret)  # For creating training and testing sets

```

# Setting up environment & paths
```{r setup}
## ------------------------------------------------
## ---- DEPENDENCIES AKA FILES REQUIRED
## Make sure that all files are in the current working directory:
## 1. chr1_bases.rda
## 2. TCGA-13-0723-01A_lib2_all_chr1.forward
## 3. helpers_file
## 
## Note that in this lecture we'll create and save a new file:
## 4. reads_gc_20K.rda


## ------------------------------------------------
## ---- CLEAN ENVIRONMENT
rm(list = ls())


## ---- SET WORKING DIRECTORY
dir_name = "~/Carmel/RProjects/Lab_Benjamini/data/"
# dir_name = "/Users/alevi/Downloads/"
# setwd(dir_name)
# getwd() # validate


## ------------------------------------------------
## ---- LOAD HELPER FUNCTIONS
helpers_file = file.path(dir_name, "help_funcs.R")
source(helpers_file)
helpers <- list(
    loadRData = loadRData,
    fitMet = fitMetrics,
    rmvOut = removeOutliersPaired,
    binBases = binBases)

```

# Loading data files 

```{r warning=FALSE}

## ------------------------------------------------
## ---- LOAD FILE OF NUCLEO_BASES (A,T,C,G)
bases_file = file.path(dir_name, "chr1_line.rda")
chr1_bases = helpers$loadRData(bases_file) # like Lecture1


## ------------------------------------------------
## ---- LOAD FILE OF READS LOCATIONS (THAT START A FRAGMENT)
  
reads_file_all = file.path(dir_name,"TCGA-13-0723-01A_lib2_all_chr1.forward")
chr1_reads = data.table::fread(reads_file_all) #like Lecture2
colnames(chr1_reads) = c("Chrom","Loc","FragLen")   


```


# Preprocessing data 

```{r message=FALSE, warning=FALSE}

## chunk should not have eval = FALSE!!
## o/w it messes up GC_20K

nreads = nrow(chr1_reads)
myLocations = as.numeric(chr1_reads$Loc)
last_read = myLocations[nreads]

binsize = 20000

## ------------------------------------------------
## ---- BINNING NUCLEO_BASES DATA
##
# per bin, count occurrences of each letter
bases_20K = binBases(chr1_bases, last_read, binsize) 

# per bin, sum occurrence counts of C & G only (GC content)
GC_20K <<- bases_20K[, 3] + bases_20K[, 4] #store as global variable

# convert GC_20K into percentages
if(any(GC_20K>1)){ # This makes sure I don't repeat the command
  GC_20K = GC_20K / binsize
}

## ------------------------------------------------
## ---- BINNING READS COVERAGE DATA

# per bin, calculate reads coverage 
reads_20K = binReads(myLocations,binsize,last_read)
save(reads_20K, GC_20K,file = file.path(dir_path=dir_name,"reads_gc_20K.rda"))


## ------------------------------------------------
## ---- CLEARING UP ENV
rm("chr1_bases","chr1_reads") # remove from memory

# load data
reads_file_20K = file.path(dir_name, "reads_gc_20K.rda")
reads_gc_20K = helpers$loadRData(reads_file_20K)
# access using `$`:
#   GC_20K = reads_gc_20K$GC_20K
#   reads_20K = reads_gc_20K$reads_20K


```



# Remove outliers

```{r}
# Remove outliers from reads and GC content data
cleaned_data = helpers$rmvOut(GC_20K, reads_20K)
GC_20K_clean = cleaned_data$x
reads_20K_clean = cleaned_data$y

```


# Q1: Segmented regression


```{r pw_regression}

# Fit initial linear model
initial_model = lm(reads_20K_clean ~ GC_20K_clean)
# Using `segmented` package for piecewise regression
seg_model = segmented(initial_model, seg.Z = ~GC_20K_clean, npsi = 2)

# Extract breakpoints
breakpoints = seg_model$psi[, "Est."]
knot1 = breakpoints[[1]]
knot2 = breakpoints[[2]]

# Plot piecewise polynomial regression
my_red=rgb(1,0,0.5,1)
plot(GC_20K_clean, reads_20K_clean, pch = 16, cex = 0.5, col = rgb(0,0,0,0.5),
     xlab = "GC Content (cleaned)", ylab = "Read Counts (cleaned)",
     main = "Piecewise Polynomial Regression")
plot(seg_model, add = TRUE, col = my_red)
abline(v = c(knot1, knot2), lty = 2, col = "darkgray", lwd = 1.5)

# Plot knots for reference
abline(v = c(knot1, knot2), lty = 2, col = "darkgray", lwd = 1.5)


```


# Q2: Residuals vs Explanatory Variable
Let's start by comparing the residuals of the piecewise polynomial model and the linear model. Ideally, residuals should be randomly distributed around zero without any discernible pattern.


```{r}

# Calculate residuals
poly_res = reads_20K_clean - predict(seg_model)

# Plot residuals for piecewise polynomial model
plot(GC_20K_clean, poly_res, pch = 16, cex = 0.5, col = rgb(0, 0, 0, 0.5),
     xlab = "GC Content (cleaned)", ylab = "Residuals",
     main = "Residuals of Piecewise Polynomial Model")

abline(h = 0, col = "darkgray", lwd = 1.5)


```
     
# Bias Assessment
Bias for Certain GC Values: If there is a noticeable pattern (e.g., residuals consistently above or below zero) in the residuals plot for certain ranges of GC values, it indicates bias. A good fit will have residuals scattered randomly around zero, without any specific trend or pattern. The goal is to have residuals that are evenly distributed around zero across all GC values, indicating that the model does not systematically over or under-predict for specific GC ranges. This will help us determine if there is any bias for certain GC values.


# Q3: Comparing to Simple Linear Regression
Now, we calculate and compare Root Mean Squared Error, Mean Absolute Error and R-squared values for both models.

```{r}

# Predictions from the models
pred_seg = predict(seg_model)

# Compute metrics
met_seg = helpers$fitMet(reads_20K_clean, pred_seg)

# Combine metrics for comparison
metrics_comparison = data.frame(
  Model = c("Segmented"),
  R_sqr = c(met_seg$R_sqr),
  MSE = c(met_seg$MSE),
  MAE = c(met_seg$MAE)
)

print(metrics_comparison)
```


```{r}
# Fit a simple linear regression model
linear_model <- lm(reads_20K_clean ~ GC_20K_clean)

# Predictions from the linear model
pred_linear <- predict(linear_model)

# Predictions from the segmented model
pred_seg <- predict(seg_model)


```


```{r}

# Helper function to compute regression metrics
compute_metrics <- function(actual, predicted) {
  mse <- mean((actual - predicted) ^ 2)
  rmse <- sqrt(mse)
  mae <- mean(abs(actual - predicted))
  rss <- sum((predicted - actual) ^ 2)
  tss <- sum((actual - mean(actual)) ^ 2)
  r_squared <- 1 - (rss / tss)
  mspe <- mean((actual - predicted) ^ 2)  # Mean Squared Prediction Error
  
  return(list(RMSE = rmse, MAE = mae, R_squared = r_squared, MSE = mse, MSPE = mspe))
}
# Combine metrics for comparison
metrics_comparison <- data.frame(
  Model = c("Linear", "Segmented"),
  R_squared = c(metrics_linear$R_squared, metrics_segmented$R_squared),
  RMSE = c(metrics_linear$RMSE, metrics_segmented$RMSE),
  MAE = c(metrics_linear$MAE, metrics_segmented$MAE),
  MSE = c(metrics_linear$MSE, metrics_segmented$MSE),
  MSPE = c(metrics_linear$MSPE, metrics_segmented$MSPE)
)

# Compute metrics
metrics_linear <- compute_metrics(reads_20K_clean, pred_linear)
metrics_segmented <- compute_metrics(reads_20K_clean, pred_seg)


print(metrics_comparison)

# Plot the actual vs predicted values for both models
plot(GC_20K_clean, reads_20K_clean, pch = 16, cex = 0.5, col = "darkblue", 
     xlab = "GC Content (cleaned)", ylab = "Read Counts (cleaned)",
     main = "Comparison of Linear and Segmented Regression Models")

# Add the linear model prediction line
lines(GC_20K_clean, pred_linear, col = "lightgreen", lwd = 2)

# Add the segmented model prediction line
plot(seg_model, add = TRUE, col = my_red, lwd = 2)

# Add a legend
legend("topleft", legend = c("Actual", "Linear Model", "Segmented Model"),
       col = c("darkblue", "lightgreen", my_red), pch = c(16, NA, NA), lwd = c(NA, 2, 2))
```
The segmented regression model provides a significantly better fit to the data compared to the linear regression model. This is evidenced by:

Higher RÂ², indicating better explanatory power.
Lower RMSE, MAE, and MSE, indicating more accurate predictions.
Residual analysis showing lower variability in residuals for the segmented model, particularly in higher GC content quantiles.
Overall, the segmented model improves the quality of predictions, capturing the underlying trends in the data more effectively than the linear model.


```{r}
# Calculate residuals for both models
residuals_linear <- reads_20K_clean - pred_linear
residuals_segmented <- reads_20K_clean - pred_seg

```

```{r}
# Plot residuals for the linear model
plot(GC_20K_clean, residuals_linear, pch = 16, cex = 0.5, col = "lightgreen",
     xlab = "GC Content (cleaned)", ylab = "Residuals",
     main = "Residuals vs GC Content (Linear Model)")
abline(h = 0, col = "red", lwd = 2.3)

# Plot residuals for the segmented model
plot(GC_20K_clean, residuals_segmented, pch = 16, cex = 0.5, col = "darkblue",
     xlab = "GC Content (cleaned)", ylab = "Residuals",
     main = "Residuals vs GC Content (Segmented Model)")
abline(h = 0, col = "red", lwd = 2.3)

```
```{r}
# Define quantiles for GC content
gc_quantiles <- quantile(GC_20K_clean, probs = c(0, 0.25, 0.5, 0.75, 1))

# Function to compute standard deviation of residuals within each quantile
compute_residual_stats <- function(gc, residuals, quantiles) {
  stats <- data.frame(
    quantile = character(),
    StdDev_Residuals = numeric()
  )
  for (i in 1:(length(quantiles) - 1)) {
    mask <- (gc >= quantiles[i]) & (gc < quantiles[i + 1])
    stddev_residuals <- sd(residuals[mask])
    stats <- rbind(stats, data.frame(
      quantile = paste0("Q", i),
      StdDev_Residuals = stddev_residuals
    ))
  }
  return(stats)
}

# Compute statistics for both models
stats_linear <- compute_residual_stats(GC_20K_clean, residuals_linear, gc_quantiles)
stats_segmented <- compute_residual_stats(GC_20K_clean, residuals_segmented, gc_quantiles)
colnames(stats_linear)[1] = "Quantile"
colnames(stats_linear)[2] = "Std Dev Res (Linear)"
colnames(stats_segmented)[2] = "Std Dev Res (Piece Wise)"
stats_linear[2] = round(stats_linear[2],2)
stats_segmented[2] = round(stats_segmented[2],2)
stats_compare = cbind(stats_linear,stats_segmented[2])
print(stats_compare)

```
The segmented model provides a better fit to the data compared to the linear model, as evidenced by:

1. Lower standard deviations in residuals across most quantiles. \n
2. A more consistent spread of residuals across the range of GC content. \n
These results suggest that the segmented model improves the prediction quality by more accurately capturing the underlying relationship between GC content and read counts, particularly in regions with higher GC content where the linear model struggles.


-------------------------------------------

# Splitting Data to Test and Train

```{r train_test}
set.seed(123)

# Create training and testing sets
train_index = createDataPartition(GC_20K_clean, p = 0.8, list = FALSE)
GC_20K_train = GC_20K_clean[train_index]
reads_20K_train = reads_20K_clean[train_index]
GC_20K_test = GC_20K_clean[-train_index]
reads_20K_test = reads_20K_clean[-train_index]
```

# Conclusion
Based on the visual comparison of residuals and numerical comparison metrics:

The model using the `segmented` package shows more evenly distributed residuals around zero, indicating a potentially better fit than the linear or the quadratic models. The MSPE, MSE and R-squared metrics further support this indication.

